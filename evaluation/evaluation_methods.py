"""
This file contains the script to run the evaluation, after the trajectories have been generated.
"""
import pickle
from collections import defaultdict
from typing import List
import time

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from tqdm import tqdm

from evaluation.evaluation_functions import EvaluationFeaturesExtractor
from evaluation.generate_trajectories import generate_trajectories
from evaluation.test_evaluation_features import get_ground_truth_data
from sim4ad.data import ScenarioConfig
from sim4ad.path_utils import get_file_name_evaluation, get_config_path


def evaluate_trajectories(policy_type, spawn_method, irl_weights, episode_names: List[str]):
    """
    Evaluate the trajectories generated by the simulator.

    :param policy_type: The policy type.
    :param spawn_method: The spawn method.
    :param irl_weights: The IRL weights.
    :param episode_nams: The episode name.
    """

    simulation_agent_features = generate_trajectories(policy_type, spawn_method, irl_weights, episode_names)
    gt_agents_data = get_ground_truth_data(episode_names)

    evaluator = EvaluationFeaturesExtractor("evaluator")
    evaluator.load(simulation_agent_features)

    evaluation_dir, filename = get_file_name_evaluation(policy_type, spawn_method, irl_weights, episode_names)

    metric_values, scores = evaluator.compute_similarity_scores(gt_agents_data=gt_agents_data, filename=filename)
    evaluation_results = {"metric_values": metric_values, "scores": scores}

    with open(evaluation_dir, "wb") as f:
        pickle.dump(evaluation_results, f)

    return evaluation_results


def run_all_evaluation_types(pol_evaluated, irl_weights_used, evaluation_eps):
    #### MICRO ANALYSIS ####
    micro_agent_features = evaluate_trajectories(policy_type=pol_evaluated, spawn_method="dataset_one",
                                                 irl_weights=irl_weights_used, episode_names=evaluation_eps)

    #### MACRO ANALYSIS ####
    macro_agent_features = evaluate_trajectories(policy_type=pol_evaluated, spawn_method="dataset_all",
                                                 irl_weights=irl_weights_used, episode_names=evaluation_eps)

    return micro_agent_features, macro_agent_features


def evaluate_all(policies_to_evaluate, evaluation_episodes, generalisation_episodes, USE_ADE=False):
    start_time = time.time()

    irl_weights = None

    # score as rows and the policies as columns
    overall_micro_analysis_scores = {}

    overall_macro_analysis_scores = {}

    overall_macro_metrics = {}

    overall_micro_metrics = {}

    for policy_to_evaluate in tqdm(policies_to_evaluate):
        micro_eval, _ = get_file_name_evaluation(policy_to_evaluate, "dataset_one", irl_weights, evaluation_episodes)
        macro_eval, _ = get_file_name_evaluation(policy_to_evaluate, "dataset_all", irl_weights, evaluation_episodes)

        with open(micro_eval, "rb") as f:
            micro_agent_features = pickle.load(f)
        with open(macro_eval, "rb") as f:
            macro_agent_features = pickle.load(f)

        overall_micro_analysis_scores[policy_to_evaluate] = micro_agent_features["scores"]
        overall_macro_analysis_scores[policy_to_evaluate] = macro_agent_features["scores"]
        overall_macro_metrics[policy_to_evaluate] = macro_agent_features["metric_values"]
        overall_micro_metrics[policy_to_evaluate] = micro_agent_features["metric_values"]

    # We use Generalisation as a score. therefore take the average of the scores and add it to the overall scores
    if generalisation_episodes is not None:
        for policy_to_evaluate in policies_to_evaluate:
            micro_eval, _ = get_file_name_evaluation(policy_to_evaluate, "dataset_one", irl_weights,
                                                     generalisation_episodes)
            macro_eval, _ = get_file_name_evaluation(policy_to_evaluate, "dataset_all", irl_weights,
                                                     generalisation_episodes)

            with open(micro_eval, "rb") as f:
                micro_agent_features_gen = pickle.load(f)
            with open(macro_eval, "rb") as f:
                macro_agent_features_gen = pickle.load(f)

            if not USE_ADE:
                # Drop the ADE and FDE scores
                original_score_gen = np.nanmean(list(micro_agent_features_gen["scores"].values()))
                micro_agent_features_gen["scores"].pop("S_ADE", None)
                micro_agent_features_gen["scores"].pop("S_FDE", None)

            micro_score_gen = np.nanmean(list(micro_agent_features_gen["scores"].values()))
            macro_score_gen = np.nanmean(list(macro_agent_features_gen["scores"].values()))

            overall_micro_analysis_scores[policy_to_evaluate]["S_Generalisation"] = micro_score_gen
            overall_macro_analysis_scores[policy_to_evaluate]["S_Generalisation"] = macro_score_gen

    # For every policy, compute the average score for the micro and macro analysis and then average these two
    # averages to get the overall score
    overall_scores = defaultdict(dict)
    for policy_to_evaluate in policies_to_evaluate:

        if not USE_ADE:
            # Drop the ADE and FDE scores
            overall_micro_analysis_scores[policy_to_evaluate].pop("S_ADE", None)
            overall_micro_analysis_scores[policy_to_evaluate].pop("S_FDE", None)
            overall_macro_analysis_scores[policy_to_evaluate].pop("S_ADE", None)
            overall_macro_analysis_scores[policy_to_evaluate].pop("S_FDE", None)

        micro_score = np.nanmean(list(overall_micro_analysis_scores[policy_to_evaluate].values()))
        macro_score = np.nanmean(list(overall_macro_analysis_scores[policy_to_evaluate].values()))

        overall_scores[policy_to_evaluate]["micro"] = micro_score
        overall_scores[policy_to_evaluate]["macro"] = macro_score
        overall_scores[policy_to_evaluate]["overall"] = (micro_score + macro_score) / 2

        # Compute the average of the scores across macro and micro
        average_scores = {}
        for score in overall_micro_analysis_scores[policy_to_evaluate].keys():
            average_scores[score] = (overall_micro_analysis_scores[policy_to_evaluate][score] +
                                     overall_macro_analysis_scores[policy_to_evaluate][score]) / 2

        # add average scores to the overall scores
        overall_scores[policy_to_evaluate].update(average_scores)

        # Fidelity: fde, ade, td-ade
        # If one of the scores is missing, we set the average to NaN
        if USE_ADE:
            if "S_FDE" not in average_scores:
                average_scores["S_FDE"] = np.nan
            if "S_ADE" not in average_scores:
                average_scores["S_ADE"] = np.nan
        if "S_TD_ADE" not in average_scores:
            average_scores["S_TD_ADE"] = np.nan
        if "S_Collision" not in average_scores:
            average_scores["S_Collision"] = np.nan
        if "S_Off_road" not in average_scores:
            average_scores["S_Off_road"] = np.nan
        if "S_Distance_nearby" not in average_scores:
            average_scores["S_Distance_nearby"] = np.nan
        if "S_Distance_right_marking" not in average_scores:
            average_scores["S_Distance_right_marking"] = np.nan
        if generalisation_episodes is not None:
            if "S_Generalisation" not in average_scores:
                average_scores["S_Generalisation"] = np.nan
        if "S_TTC" not in average_scores:
            average_scores["S_TTC"] = np.nan
        if "S_TTH" not in average_scores:
            average_scores["S_TTH"] = np.nan
        if "S_Velocity" not in average_scores:
            average_scores["S_Velocity"] = np.nan
        if "S_Interference" not in average_scores:
            average_scores["S_Interference"] = np.nan

        # average S_FDE, S_ADE, S_TD_ADE WITHOUT counting the NaN values
        if USE_ADE:
            overall_scores[policy_to_evaluate]["S_Fidelity"] = np.nanmean(
                [average_scores["S_FDE"], average_scores["S_ADE"], average_scores["S_TD_ADE"]])
        else:
            overall_scores[policy_to_evaluate]["S_Fidelity"] = np.nanmean([average_scores["S_TD_ADE"]])

        # Safety: collision_rate, off_road_rate, distance_to_neighbour
        # (average_scores["S_Collision"] + average_scores["S_Off_road"] + average_scores["S_Distance_nearby"]) / 3
        overall_scores[policy_to_evaluate]["S_Safety"] = np.nanmean(
            [average_scores["S_Collision"], average_scores["S_Off_road"], average_scores["S_Distance_nearby"]])

        # Diversity: distance_right_marking, s_Generalisation
        # (average_scores["S_Distance_right_marking"] + average_scores["S_Generalisation"]) / 2
        if generalisation_episodes is not None:
            overall_scores[policy_to_evaluate]["S_Diversity"] = np.nanmean(
                [average_scores["S_Distance_right_marking"], average_scores["S_Generalisation"]])
        else:
            overall_scores[policy_to_evaluate]["S_Diversity"] = average_scores["S_Distance_right_marking"]

        # Comfort: TTC, TTH, Velocity, Interference
        # (average_scores["S_TTC"] + average_scores["S_TTH"] + average_scores["S_Velocity"] + average_scores["S_Interference"]) / 4
        overall_scores[policy_to_evaluate]["S_Comfort"] = np.nanmean(
            [average_scores["S_TTC"], average_scores["S_TTH"], average_scores["S_Velocity"],
             average_scores["S_Interference"]])

    overall_scores["all_micro"] = overall_micro_analysis_scores
    overall_scores["all_macro"] = overall_macro_analysis_scores
    overall_scores["all_micro_metrics"] = overall_micro_metrics
    overall_scores["all_macro_metrics"] = overall_macro_metrics

    with open(f"evaluation/results/evaluation_results_all.pkl", "wb") as f:
        pickle.dump(overall_scores, f)

    print(f"Time taken: {time.time() - start_time} seconds")


def generate_latex_table(results, map):
    micro_metrics = results["all_micro_metrics"]
    macro_metrics = results["all_macro_metrics"]
    micr_scores = results["all_micro"]
    macro_scores = results["all_macro"]

    # dataframe from micro_metrics
    micro_df = pd.DataFrame(micro_metrics)
    macro_df = pd.DataFrame(macro_metrics)
    miicro_scores_df = pd.DataFrame(micr_scores)
    macro_scores_df = pd.DataFrame(macro_scores)

    def latex(df, type, map, metrics, caption=None):
        df = df.round(2)

        if metrics:
            df.rename(index={"Interference": "Vehicle interference"}, inplace=True)
            df.rename(index={"Distance_nearby": "Distance to neighbours"}, inplace=True)
            df.rename(index={"Distance_right_marking": "Distance to markings"}, inplace=True)
            df.rename(index={"JSD_Velocity": "JSD speed distribution"}, inplace=True)
            df.rename(index={"Off_road_rate": "Off road rate"}, inplace=True)
            df.rename(index={"Collision_rate": "Collision rate"}, inplace=True)
            df.rename(index={"JSD_TTC": "JSD TTC"}, inplace=True)
            df.rename(index={"JSD_TTH": "JSD TTH"}, inplace=True)
        else:
            df.rename(index={"S_ADE": "ADE", "S_FDE": "FDE", "S_TD_ADE": "TD-ADE", "S_Collision": "Collision",
                             "S_Off_road": "Off road", "S_Distance_nearby": "Distance to neighbours",
                             "S_Distance_right_marking": "Distance to markings", "S_Generalisation": "Generalisation",
                             "S_TTC": "TTC", "S_TTH": "TTH", "S_Velocity": "Speed distribution",
                             "S_Interference": "Vehicle interference"}, inplace=True)

        names = {"sac_5_rl": "SAC", "bc-all-obs-5_pi_cluster_all": "BC", "idm": "IDM"}
        df.rename(columns=names, inplace=True)

        if caption is not None:
            caption = caption
        else:
            if metrics:
                caption = f"Evaluation metrics for the {type} experiment in {map}. "
            else:
                caption = f"Similarity scores for the {type} experiment in {map}"
        label = f"tab:{type}_{map}"
        latex_table = df.to_latex(index=True, header=True, column_format='||c | c | c | c | c||', caption=caption,
                                  float_format="%.2f", label=label)
        latex_table = latex_table.replace("\\begin{tabular}", "\\begin{center}\n\\begin{tabular}")
        latex_table = latex_table.replace("\\end{tabular}", "\\end{tabular}\n\\end{center}")
        return latex_table

    print(latex(micro_df, type="micro", map=map, metrics=True))
    print(latex(macro_df, type="macro", map=map, metrics=True))
    print(latex(miicro_scores_df, type="micro", map=map, metrics=False))
    print(latex(macro_scores_df, type="macro", map=map, metrics=False))

    # average values between micro_df and macro_df
    average_df = (micro_df + macro_df) / 2
    print(latex(average_df, type="average", map=map, metrics=True,
                caption=f"Average metrics for the evaluation in {map}"))


def get_results(policies_to_evaluate, map_to_use):
    with open(f"evaluation/results/evaluation_results_all.pkl", "rb") as f:
        results = pickle.load(f)

    # Use a radar plot to visualise how the plicies compare across all metrics, e.g., s_fde,s_micro
    radar_data = {}
    for policy in policies_to_evaluate:
        if "cluster" in policy and "cluster_all" not in policy:
            continue
        radar_data[policy] = results[policy]

    def radar_plot_features(radar_data, policies):
        # Categories are now all the metrics score individually
        N = None
        categories = None
        for policy in policies:
            if "cluster" in policy and "cluster_all" not in policy:
                continue
            categories = list(radar_data[policy].keys())
            categories = [x.replace("S_", "") for x in categories]
            categories = [x.lower() for x in categories]
            categories = [f"s_{x}" for x in categories]

            # substitute the categories with the following
            # s_distance_right_marking with s_d_marking

            categories = [x.replace("s_distance_right_marking", "s_d_markings") for x in categories]
            categories = [x.replace("s_distance_nearby", "s_d_neighbours") for x in categories]
            categories = [x.replace("s_comfort", "s_ba") for x in categories]
            categories = [x.replace("s_diversity", "s_diversity") for x in categories]
            categories = [x.replace("s_safety", "s_safety") for x in categories]
            categories = [x.replace("s_fidelity", "s_fidelity") for x in categories]
            categories = [x.replace("s_ttc", "s_TTC") for x in categories]
            categories = [x.replace("s_tth", "s_TTH") for x in categories]
            categories = [x.replace("s_ade", "s_ADE") for x in categories]
            categories = [x.replace("s_fde", "s_FDE") for x in categories]
            categories = [x.replace("s_td_ade", "s_TD-ADE") for x in categories]
            categories = [x.replace("s_velocity", "s_speed_distribution") for x in categories]
            categories = [x.replace("s_interference", "s_vehicle_interference") for x in categories]
            categories = [x.replace("s_micro", "s_micro") for x in categories]
            categories = [x.replace("s_macro", "s_macro") for x in categories]
            categories = [x.replace("s_overall", "s_overall") for x in categories]

            # remove all s_
            # categories = [x.replace("s_", "") for x in categories]

            if N is None:
                N = len(categories)
        angles = [n / float(N) * 2 * np.pi for n in range(N)]
        angles += angles[:1]

        fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(polar=True))

        plt.xticks(angles[:-1], categories, size=15)
        ax.set_rlabel_position(0)

        for policy in policies:
            if "cluster" in policy and "cluster_all" not in policy:
                continue
            values = list(radar_data[policy].values())
            values += values[:1]
            # make the plot line lighter and DASHED
            labels = {"sac_5_rl": "SAC", "bc-all-obs-5_pi_cluster_Aggressive": "BC Aggressive",
                      "bc-all-obs-5_pi_cluster_all": "BC", "idm": "IDM", "bc-all-obs-5_pi_cluster_Normal": "BC Normal",
                      "bc-all-obs-5_pi_cluster_Cautious": "BC Cautious"}
            ax.plot(angles, values, linewidth=1, linestyle='solid', label=labels[policy])
            ax.fill(angles, values, alpha=0.1)

        plt.legend(loc='upper right', fontsize=15)

        plt.title(f"Scores for the evaluation in {map_to_use}", size=20)
        plt.savefig(f"evaluation/results/radar_plot_{map_to_use}_{'s_ADE' in categories}ADE.svg", format="svg")
        plt.show()

    radar_plot_features(radar_data, policies_to_evaluate)

    return results


def main():
    RUN = True
    policies_to_evaluate = ['results/offlineRL/checkpoint']  # ["bc-all-obs-5_pi_cluster_General"]
    normal_map = "appershofen"
    generalisation_map = "hausen"

    configs = ScenarioConfig.load(get_config_path(normal_map))
    idx = configs.dataset_split["test"]
    evaluation_episodes = [x.recording_id for i, x in enumerate(configs.episodes) if i in idx]

    generalisation_configs = ScenarioConfig.load(get_config_path(generalisation_map))
    idx = generalisation_configs.dataset_split["test"]
    generalisation_episodes = [x.recording_id for i, x in enumerate(generalisation_configs.episodes) if i in idx]

    if RUN:
        for policy in policies_to_evaluate:
            run_all_evaluation_types(policy, None, evaluation_episodes)
            run_all_evaluation_types(policy, None, generalisation_episodes)

    for use_ade in [True]:
        for to_use in [normal_map, generalisation_map]:
            if to_use == "appershofen":
                evaluate_all(policies_to_evaluate, evaluation_episodes, None, USE_ADE=use_ade)
            else:
                evaluate_all(policies_to_evaluate, generalisation_episodes, None, USE_ADE=use_ade)
            results = get_results(policies_to_evaluate, to_use)
            print(generate_latex_table(results, map=to_use))


if __name__ == "__main__":
    main()
