from sim4ad.path_utils import get_config_path
from sim4ad.util import parse_args
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
import seaborn as sns

from sim4ad.data.data_loaders import DatasetDataLoader
from sim4ad.clustering.visualization import Visualization


class FeatureExtraction:
    """
    features determined by the paper "Feature selection for driving style and skill clustering using naturalistic
    driving data and driving behavior questionnaire"

    also refer to the paper "Classifying travelersâ€™ driving style using basic safety messages generated by connected
    vehicles: Application of unsupervised machine learning"
    """

    def __init__(self, episodes):
        self.features_one = {'episode_id': [], 'agent_id': [], 'long_acc_max': [], 'long_acc_fc': [], 'lat_acc_sf': [],
                            'lat_acc_rms': [], 'vel_std': [], 'label': None}
        self.features_two = {'episode_id': [], 'agent_id': [], 'speed_vf': [], 'speed_dmean': [], 'speed_cv': [],
                            'speed_qcv': [], 'accx_dmean': [], 'accy_dmean': [], 'label': None}
        self.episodes = episodes
        self.windows_length = 3  # unit: s

    @staticmethod
    def get_root_mean_square(data):
        N = len(data)
        # Root-mean-square
        data_rms = np.sqrt(sum([data[i] ** 2 for i in range(N)]) / N)

        return data_rms

    @staticmethod
    def get_average_rectified_value(data):
        N = len(data)
        # Average rectified value
        lat_acc_arv = sum([abs(data[i]) for i in range(N)]) / N

        return lat_acc_arv

    def get_shape_factor(self, data):
        # Shape factor of lateral acceleration
        # Root-mean-square
        data_rms = self.get_root_mean_square(data)
        # Average rectified value
        data_arv = self.get_average_rectified_value(data)

        # Shape factor
        data_sf = data_rms / data_arv

        return data_sf

    @staticmethod
    def get_standard_deviation(data):
        return np.std(data)

    @staticmethod
    def get_frequency_centroid(data):
        N = len(data)
        # sample frequency obtained from the dataset
        sampling_rate = 30
        dft = np.fft.fft(data)
        magnitude = np.abs(dft)
        freq = np.linspace(0, sampling_rate, N)
        numerator = np.sum(magnitude * freq)
        denominator = np.sum(magnitude)
        frequency_centroid = numerator / denominator if denominator != 0 else 0

        return frequency_centroid

    @staticmethod
    def get_max_value(data):
        return np.max(data)

    @staticmethod
    def get_coefficient_variation(data):
        data_dev = np.std(data)
        data_mean = np.mean(data)
        return data_dev / abs(data_mean) * 100

    @staticmethod
    def get_mean_absolute_deviation(data):
        data_mean = np.mean(data)
        return sum(abs(d - data_mean) for d in data) / len(data)

    @staticmethod
    def get_quartile_coefficient_variation(data):
        q1 = np.percentile(data, 25)
        q3 = np.percentile(data, 75)
        return (q3 - q1) / (q3 + q1) * 100

    @staticmethod
    def get_time_varying_stochastic_volatility(data):
        r = []
        for i in range(len(data) - 1):
            r.append(np.log(data[i + 1] / data[i]) * 100)

        r_mean = np.mean(r)
        return np.sqrt(sum((r_ - r_mean) ** 2 for r_ in r) / (len(r) - 1))

    def extract_features(self, feature_selection: int = 1) -> pd.DataFrame:
        """Extract feature values from the dataset for clustering"""
        df = None
        for episode in self.episodes:
            for agent_id, agent in episode.agents.items():
                time = agent.time
                long_acc = agent.ax_vec
                lat_acc = agent.ay_vec
                velocity = [np.sqrt(agent.vx_vec[i] ** 2 + agent.vy_vec[i] ** 2) for i in range(len(agent.vx_vec))]

                if feature_selection == 1:
                    # get feature values
                    self.features_one['episode_id'].append(episode.config.recording_id)
                    self.features_one['agent_id'].append(agent_id)
                    self.features_one['long_acc_max'].append(self.get_max_value(long_acc))
                    self.features_one['long_acc_fc'].append(self.get_frequency_centroid(long_acc))
                    self.features_one['lat_acc_sf'].append(self.get_shape_factor(lat_acc))
                    self.features_one['lat_acc_rms'].append(self.get_root_mean_square(lat_acc))
                    self.features_one['vel_std'].append(self.get_standard_deviation(velocity))

                elif feature_selection == 2:
                    initial_time = time[0]
                    initial_inx = 0
                    speed_vf = []
                    speed_dmean = []
                    speed_cv = []
                    speed_qcv = []
                    accx_dmean = []
                    accy_dmean = []
                    for inx, t in enumerate(time):
                        if t - initial_time >= self.windows_length:
                            vel = velocity[initial_inx:inx]
                            speed_vf.append(self.get_time_varying_stochastic_volatility(vel))
                            speed_dmean.append(self.get_mean_absolute_deviation(vel))
                            speed_cv.append(self.get_coefficient_variation(vel))
                            speed_qcv.append(self.get_quartile_coefficient_variation(vel))
                            accx_dmean.append(self.get_mean_absolute_deviation(long_acc[initial_inx:inx]))
                            accy_dmean.append((self.get_mean_absolute_deviation(lat_acc[initial_inx:inx])))
                            # reset the initial time until another windows length
                            initial_time = t
                            initial_inx = inx

                    self.features_two['episode_id'].append(episode.config.recording_id)
                    self.features_two['agent_id'].append(agent_id)
                    # compute average value for clustering
                    self.features_two["speed_vf"].append(np.average(speed_vf))
                    self.features_two["speed_dmean"].append(np.average(speed_dmean))
                    self.features_two["speed_cv"].append(np.average(speed_cv))
                    self.features_two["speed_qcv"].append(np.average(speed_qcv))
                    self.features_two["accx_dmean"].append(np.average(accx_dmean))
                    self.features_two["accy_dmean"].append(np.average(accy_dmean))

        if feature_selection == 1:
            df = pd.DataFrame(self.features_one)
        elif feature_selection == 2:
            df = pd.DataFrame(self.features_two)

        return df


class Clustering:
    """
    Various clustering methods can be defined here
    Normal, cautious, and aggressive drivers are defined here
    """

    def __init__(self):
        self._n_cluster = 3

    def kmeans(self, dataframe):
        scaler = StandardScaler()
        scaled_features = scaler.fit_transform(dataframe.iloc[:, 2:-1])
        # replace 3 with your chosen number of clusters
        kmeans = KMeans(n_init=10, n_clusters=self._n_cluster, random_state=0)
        kmeans.fit(scaled_features)
        dataframe['label'] = kmeans.labels_
        cluster_centers = kmeans.cluster_centers_

        return dataframe, cluster_centers

    def hierarchical(self, dataframe):
        scaler = StandardScaler()
        scaled_features = scaler.fit_transform(dataframe.iloc[:, 2:-1])

        # Hierarchical clustering
        hc = AgglomerativeClustering(n_clusters=self._n_cluster, metric='euclidean', linkage='ward')
        hc.fit(scaled_features)
        dataframe['label'] = hc.labels_
        cluster_centers = np.array([scaled_features[hc.labels_ == i].mean(axis=0) for i in range(hc.n_clusters)])

        # Plotting dendrogram
        linked = linkage(scaled_features, method='ward')
        plt.figure(figsize=(10, 7))
        dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)
        plt.title('Hierarchical Clustering Dendrogram')
        plt.xlabel('Sample index')
        plt.ylabel('Distance')

        return dataframe, cluster_centers

    def GMM(self, dataframe):
        scaler = StandardScaler()
        scaled_features = scaler.fit_transform(dataframe.iloc[:, 2:-1])

        # Create a Gaussian Mixture Model
        gmm = GaussianMixture(n_components=self._n_cluster, random_state=0)
        # Fit the model
        gmm.fit(scaled_features)
        # Predict the cluster for each data point
        dataframe['label'] = gmm.predict(scaled_features)
        means = gmm.means_

        return dataframe, means

    @staticmethod
    def evaluation(clustered_dataframe):
        """Evaluate the clustered trajectories using some metrics
            Silhouette Score, Davies-Bouldin Index, Calinski-Harabasz Index
        """
        silhouette = silhouette_score(clustered_dataframe.iloc[:, 2:-1], clustered_dataframe.iloc[:, -1])
        dbi = davies_bouldin_score(clustered_dataframe.iloc[:, 2:-1], clustered_dataframe.iloc[:, -1])
        chi = calinski_harabasz_score(clustered_dataframe.iloc[:, 2:-1], clustered_dataframe.iloc[:, -1])

        return silhouette, dbi, chi


def plot_features(y_label, clustered_dataframe):
    """Plot the feature value for clustered trajectories

       Args:
            y_label: the name of the feature
            clustered_dataframe: The entire data after clustering
    """
    plt.figure(figsize=(8, 6))
    # Create a boxplot
    sns.boxplot(x='label', y=y_label, data=clustered_dataframe)
    # Display the plot
    plt.title('Boxplot Grouped by Cluster')
    plt.xlabel('Cluster')
    plt.ylabel(y_label)


def plot_radar_charts(feature_names, cluster_centers):
    """Plot the radar chart for each scaled cluster center"""
    # Compute angle each bar is centered on:
    angles = np.linspace(0, 2 * np.pi, len(feature_names), endpoint=False).tolist()

    values = np.concatenate((cluster_centers, [cluster_centers[0]]))
    angles += angles[:1]

    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))
    ax.fill(angles, values, color='red', alpha=0.25)
    ax.plot(angles, values, color='red', linewidth=2)

    ax.set_yticklabels([])
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(feature_names)


def plot_clustered_trj_on_map(data_loader, clustered_dataframe):
    """Plot the clustered trajectory on the map"""
    visual = Visualization()
    # first episode, please change accordingly
    visual.plot_clustered_trj_on_map(data_loader.scenario.episodes[0], clustered_dataframe)


def main():
    args = parse_args()
    data_loader = DatasetDataLoader(get_config_path(args.map))
    data_loader.load()

    feature_extractor = FeatureExtraction(data_loader.scenario.episodes)
    # determine which features category is used
    feature_selection = 1
    # extract feature values from the dataset
    df = feature_extractor.extract_features(feature_selection=feature_selection)

    # begin the clustering
    cluster = Clustering()
    if args.clustering == 'kmeans':
        clustered_dataframe, cluster_centers = cluster.kmeans(df)
    elif args.clustering == 'hierarchical':
        clustered_dataframe, cluster_centers = cluster.hierarchical(df)
    elif args.clustering == 'gmm':
        clustered_dataframe, cluster_centers = cluster.GMM(df)
    else:
        raise 'No clustering method is specified.'

    grouped_cluster = clustered_dataframe.groupby('label')

    # calculate important metrics
    silhouette, dbi, chi = cluster.evaluation(clustered_dataframe)
    print('silhouette value is ', silhouette)
    print('Davies-Bouldin Index is ', dbi)
    print('Calinski-Harabasz Index is ', chi)

    # visualize each feature values after clustering
    if feature_selection == 1:
        feature_names = list(feature_extractor.features_one.keys())
        feature_names = feature_names[2:-1]
        for feature_name in feature_names:
            plot_features(feature_name, clustered_dataframe)
        plt.show()

    elif feature_selection == 2:
        feature_names = list(feature_extractor.features_two.keys())
        feature_names = feature_names[2:-1]
        for cluster_center in cluster_centers:
            plot_radar_charts(feature_names, cluster_center)
        plt.show()


if __name__ == '__main__':
    sys.exit(main())
