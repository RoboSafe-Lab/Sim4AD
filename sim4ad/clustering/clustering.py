from sim4ad.path_utils import get_config_path
from sim4ad.util import parse_args
import sys
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
import seaborn as sns

from sim4ad.data.data_loaders import DatasetDataLoader
from sim4ad.clustering.visualization import Visualization


class FeatureExtraction:
    """
    features determined by the paper "Feature selection for driving style and skill clustering using naturalistic
    driving data and driving behavior questionnaire"

    also refer to the paper "Classifying travelersâ€™ driving style using basic safety messages generated by connected
    vehicles: Application of unsupervised machine learning"
    """

    def __init__(self):
        self.features_one = {'episode_id': [], 'agent_id': [], 'long_acc_max': [], 'long_acc_fc': [], 'lat_acc_sf': [],
                             'lat_acc_rms': [], 'vel_std': [], 'label': None}
        self.features_two = {'episode_id': [], 'agent_id': [], 'speed_vf': [], 'speed_dmean': [], 'speed_cv': [],
                             'speed_qcv': [], 'accx_dmean': [], 'accy_dmean': [], 'label': None}

        self.windows_length = 3  # unit: s

    @staticmethod
    def get_root_mean_square(data):
        N = len(data)
        # Root-mean-square
        data_rms = np.sqrt(sum([data[i] ** 2 for i in range(N)]) / N)

        return data_rms

    @staticmethod
    def get_average_rectified_value(data):
        N = len(data)
        # Average rectified value
        lat_acc_arv = sum([abs(data[i]) for i in range(N)]) / N

        return lat_acc_arv

    def get_shape_factor(self, data):
        # Shape factor of lateral acceleration
        # Root-mean-square
        data_rms = self.get_root_mean_square(data)
        # Average rectified value
        data_arv = self.get_average_rectified_value(data)

        # Shape factor
        data_sf = data_rms / data_arv

        return data_sf

    @staticmethod
    def get_standard_deviation(data):
        return np.std(data)

    @staticmethod
    def get_frequency_centroid(data):
        N = len(data)
        # sample frequency obtained from the dataset
        sampling_rate = 30
        dft = np.fft.fft(data)
        magnitude = np.abs(dft)
        freq = np.linspace(0, sampling_rate, N)
        numerator = np.sum(magnitude * freq)
        denominator = np.sum(magnitude)
        frequency_centroid = numerator / denominator if denominator != 0 else 0

        return frequency_centroid

    @staticmethod
    def get_max_value(data):
        return np.max(data)

    @staticmethod
    def get_coefficient_variation(data):
        data_dev = np.std(data)
        data_mean = np.mean(data)
        return data_dev / abs(data_mean) * 100

    @staticmethod
    def get_mean_absolute_deviation(data):
        data_mean = np.mean(data)
        return sum(abs(d - data_mean) for d in data) / len(data)

    @staticmethod
    def get_quartile_coefficient_variation(data):
        q1 = np.percentile(data, 25)
        q3 = np.percentile(data, 75)
        return (q3 - q1) / (q3 + q1) * 100

    @staticmethod
    def get_time_varying_stochastic_volatility(data):
        r = []
        for i in range(len(data) - 1):
            r.append(np.log(data[i + 1] / data[i]) * 100)

        r_mean = np.mean(r)
        return np.sqrt(sum((r_ - r_mean) ** 2 for r_ in r) / (len(r) - 1))

    def get_feature_array(self, vel, long_acc, lat_acc, results_containers):
        """
        Processes a segment of the data and appends the results to the provided containers.

        Parameters:
        - initial_index: The start index of the segment.
        - end_index: The end index of the segment.
        - velocity, long_acc, lat_acc: The data arrays.
        - results_containers: A list of containers to append the results to.
        """
        results_containers[0].append(self.get_time_varying_stochastic_volatility(vel))
        results_containers[1].append(self.get_mean_absolute_deviation(vel))
        results_containers[2].append(self.get_coefficient_variation(vel))
        results_containers[3].append(self.get_quartile_coefficient_variation(vel))
        results_containers[4].append(self.get_mean_absolute_deviation(long_acc))
        results_containers[5].append(self.get_mean_absolute_deviation(lat_acc))

    def extract_features(self, episode, feature_selection: int = 1):
        """Extract feature values from the dataset for clustering"""
        for agent_id, agent in episode.agents.items():
            time = agent.time
            long_acc = agent.ax_vec
            lat_acc = agent.ay_vec
            velocity = [np.sqrt(agent.vx_vec[i] ** 2 + agent.vy_vec[i] ** 2) for i in range(len(agent.vx_vec))]

            if feature_selection == 1:
                # get feature values
                self.features_one['episode_id'].append(episode.config.recording_id)
                self.features_one['agent_id'].append(agent_id)
                self.features_one['long_acc_max'].append(self.get_max_value(long_acc))
                self.features_one['long_acc_fc'].append(self.get_frequency_centroid(long_acc))
                self.features_one['lat_acc_sf'].append(self.get_shape_factor(lat_acc))
                self.features_one['lat_acc_rms'].append(self.get_root_mean_square(lat_acc))
                self.features_one['vel_std'].append(self.get_standard_deviation(velocity))

            elif feature_selection == 2:
                initial_inx = 0
                speed_vf, speed_dmean, speed_cv, speed_qcv, accx_dmean, accy_dmean = ([] for _ in range(6))
                results_containers = [speed_vf, speed_dmean, speed_cv, speed_qcv, accx_dmean, accy_dmean]
                for inx, t in enumerate(time):
                    if t - time[initial_inx] >= self.windows_length:
                        self.get_feature_array(velocity[initial_inx:inx], long_acc[initial_inx:inx],
                                               lat_acc[initial_inx:inx], results_containers)
                        # reset the initial time until another windows length
                        initial_inx = inx

                # the total lifetime is smaller than the windows length
                if initial_inx == 0:
                    self.get_feature_array(velocity, long_acc, lat_acc, results_containers)

                self.features_two['episode_id'].append(episode.config.recording_id)
                self.features_two['agent_id'].append(agent_id)
                # compute average value for clustering
                self.features_two["speed_vf"].append(np.average(results_containers[0]))
                self.features_two["speed_dmean"].append(np.average(results_containers[1]))
                self.features_two["speed_cv"].append(np.average(results_containers[2]))
                self.features_two["speed_qcv"].append(np.average(results_containers[3]))
                self.features_two["accx_dmean"].append(np.average(results_containers[4]))
                self.features_two["accy_dmean"].append(np.average(results_containers[5]))


class Clustering:
    """
    Various clustering methods can be defined here
    Normal, cautious, and aggressive drivers are defined here
    """

    def __init__(self):
        self._n_cluster = 3

    def kmeans(self, dataframe):
        scaler = StandardScaler()
        scaled_features = scaler.fit_transform(dataframe.iloc[:, 2:-1])
        # replace 3 with your chosen number of clusters
        kmeans = KMeans(n_init='auto', n_clusters=self._n_cluster, random_state=0)
        kmeans.fit(scaled_features)
        dataframe['label'] = kmeans.labels_
        cluster_centers = kmeans.cluster_centers_

        return dataframe, cluster_centers

    def hierarchical(self, dataframe):
        scaler = StandardScaler()
        scaled_features = scaler.fit_transform(dataframe.iloc[:, 2:-1])

        # Hierarchical clustering
        hc = AgglomerativeClustering(n_clusters=self._n_cluster, metric='euclidean', linkage='ward')
        hc.fit(scaled_features)
        dataframe['label'] = hc.labels_
        cluster_centers = np.array([scaled_features[hc.labels_ == i].mean(axis=0) for i in range(hc.n_clusters)])

        # Plotting dendrogram
        linked = linkage(scaled_features, method='ward')
        plt.figure(figsize=(10, 7))
        dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)
        plt.title('Hierarchical Clustering Dendrogram')
        plt.xlabel('Sample index')
        plt.ylabel('Distance')

        return dataframe, cluster_centers

    def GMM(self, dataframe):
        scaler = StandardScaler()
        scaled_features = scaler.fit_transform(dataframe.iloc[:, 2:-1])

        # Create a Gaussian Mixture Model
        gmm = GaussianMixture(n_components=self._n_cluster, random_state=0)
        # Fit the model
        gmm.fit(scaled_features)
        # Predict the cluster for each data point
        dataframe['label'] = gmm.predict(scaled_features)
        means = gmm.means_

        return dataframe, means

    @staticmethod
    def evaluation(clustered_dataframe):
        """Evaluate the clustered trajectories using some metrics
            Silhouette Score: A value close to +1 indicates that the samples are far away from neighboring clusters,
                              while a value close to 0 indicates that the clusters are overlapping
                              A negative value indicates that samples might have been assigned to the wrong cluster
            Davies-Bouldin Index: Lower values mean better separation between clusters.
                                  A value of 0 indicates the lowest possible score
            Calinski-Harabasz Index:  Higher scores are better, as they indicate that the clusters are dense
                                      and well separated, which corresponds to a model with distinct clusters
        """
        silhouette = silhouette_score(clustered_dataframe.iloc[:, 2:-1], clustered_dataframe.iloc[:, -1])
        dbi = davies_bouldin_score(clustered_dataframe.iloc[:, 2:-1], clustered_dataframe.iloc[:, -1])
        chi = calinski_harabasz_score(clustered_dataframe.iloc[:, 2:-1], clustered_dataframe.iloc[:, -1])

        return silhouette, dbi, chi


def plot_features(y_label, clustered_dataframe):
    """Plot the feature value for clustered trajectories

       Args:
            y_label: the name of the feature
            clustered_dataframe: The entire data after clustering
    """
    plt.figure(figsize=(8, 6))
    # Create a boxplot
    sns.boxplot(x='label', y=y_label, data=clustered_dataframe)
    # Display the plot
    plt.title('Boxplot Grouped by Cluster')
    plt.xlabel('Cluster')
    plt.ylabel(y_label)


def plot_radar_charts(feature_names, inx, cluster_centers):
    """Plot the radar chart for each scaled cluster center"""
    # Compute angle each bar is centered on:
    angles = np.linspace(0, 2 * np.pi, len(feature_names), endpoint=False).tolist()

    values = np.concatenate((cluster_centers, [cluster_centers[0]]))
    angles += angles[:1]

    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))
    ax.fill(angles, values, color='red', alpha=0.25)
    ax.plot(angles, values, color='red', linewidth=2)

    ax.set_yticklabels([])
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(feature_names)
    ax.set_title(f'Cluster {inx}')


def plot_clustered_trj_on_map(data_loader, clustered_dataframe):
    """Plot the clustered trajectory on the map"""
    visual = Visualization()
    # first episode, please change accordingly
    visual.plot_clustered_trj_on_map(data_loader.scenario.episodes[0], clustered_dataframe)


def main():
    args = parse_args()
    data_loader = DatasetDataLoader(get_config_path(args.map))
    data_loader.load()

    # determine which features category is used
    feature_selection = 1
    feature_extractor = FeatureExtraction()
    # begin the clustering
    cluster = Clustering()

    # Traverse all episodes if they belong to the same map
    for episode in data_loader.scenario.episodes:
        # extract feature values from the dataset
        feature_extractor.extract_features(episode=episode, feature_selection=feature_selection)

    if feature_selection == 1:
        df = pd.DataFrame(feature_extractor.features_one)
    elif feature_selection == 2:
        df = pd.DataFrame(feature_extractor.features_two)
    else:
        df = None

    if args.clustering == 'kmeans':
        clustered_dataframe, cluster_centers = cluster.kmeans(df)
    elif args.clustering == 'hierarchical':
        clustered_dataframe, cluster_centers = cluster.hierarchical(df)
    elif args.clustering == 'gmm':
        clustered_dataframe, cluster_centers = cluster.GMM(df)
    else:
        raise 'No clustering method is specified.'

    # calculate important metrics for evaluation
    silhouette, dbi, chi = cluster.evaluation(clustered_dataframe)
    print('silhouette value is ', silhouette)
    print('Davies-Bouldin Index is ', dbi)
    print('Calinski-Harabasz Index is ', chi)

    # visualize each feature values after clustering
    if feature_selection == 1:
        feature_names = list(feature_extractor.features_one.keys())
        feature_names = feature_names[2:-1]
        for feature_name in feature_names:
            plot_features(feature_name, clustered_dataframe)
        plt.show()

    elif feature_selection == 2:
        feature_names = list(feature_extractor.features_two.keys())
        feature_names = feature_names[2:-1]
        for inx, cluster_center in enumerate(cluster_centers):
            plot_radar_charts(feature_names, inx, cluster_center)
        plt.show()

    # save clustered data after observing the radar charts
    grouped_cluster = clustered_dataframe.groupby('label')
    labeled_aid = {'Aggressive': list(grouped_cluster.get_group(2)['agent_id']),
                   'Normal': list(grouped_cluster.get_group(0)['agent_id']),
                   'Conservative': list(grouped_cluster.get_group(1)['agent_id'])}
    num = [len(labeled_aid['Aggressive']), len(labeled_aid['Normal']), len(labeled_aid['Conservative'])]
    print(f'Aggressive drivers number: {num[0]}')
    print(f'Normal drivers number: {num[1]}')
    print(f'Conservative drivers number: {num[2]}')

    json_str = json.dumps(labeled_aid, indent=4)
    file_path = 'scenarios/configs/'
    with open(file_path + 'drivingStyle.json', 'w') as json_file:
        json_file.write(json_str)
    print('Driving styles saved!')


if __name__ == '__main__':
    sys.exit(main())
